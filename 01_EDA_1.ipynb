{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Reddit Post Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Pulling information and classifying posts via Pushshift's API</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author: Brendan McDonnell**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: EDA Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring, visualizing, and pulling information from the two datasets before modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative Links\n",
    "- [Importing Libraries and Datasets Needed](#Importing-Libraries-and-Datasets-Needed)\n",
    "- [Visualizing and Exploring Data](#Visualizing-and-Exploring-Data)\n",
    "- [A Few Notes About the Data](#A-Few-Notes-About-the-Data)\n",
    "- [Export the Data to CSV](#Export-the-Data-to-CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries and Datasets Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_don = pd.read_csv('data/the_donald.csv')\n",
    "df_rep = pd.read_csv('data/republican.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing and Exploring Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_don.drop(columns='Unnamed: 0', inplace=True)\n",
    "df_rep.drop(columns='Unnamed: 0', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15,000 posts from r/The_Donald\n",
    "df_don.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14,999 posts from r/Republican\n",
    "df_rep.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nulls in body; no body to report\n",
    "# nulls in user, user has deleted account\n",
    "df_don.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nulls in body; no body to report\n",
    "# nulls in user, user has deleted account\n",
    "df_rep.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of users posting all 15,000 posts\n",
    "len(df_don.user.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of users posting all 14,999 posts\n",
    "len(df_rep.user.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 85 overlapping users posting (because nan is one of the users)\n",
    "# guessing users posting can be a good variable for narrowing down where the post is going\n",
    "len(set(df_rep.user.unique()).intersection(list(df_don.user.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# republican data starts at July 4, 2019 and goes back to May 15, 2018\n",
    "df_rep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Donald data starts at July 4, 2019 and goes back to June 28, 2019\n",
    "df_don.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_don = [1 if 'breitbart' in value else 0 for value in list(df_don['url'].values)]\n",
    "list_rep = [1 if 'breitbart' in value else 0 for value in list(df_rep['url'].values)]\n",
    "sum(list_don), sum(list_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_don = [1 if 'trump' in title.lower() else 0 for title in list(df_don['title'].values)]\n",
    "trump_rep = [1 if 'trump' in title.lower() else 0 for title in list(df_rep['title'].values)]\n",
    "sum(trump_don) / 15000, sum(trump_rep) / 14999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Few Notes About the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. r/Republican has 3,012 unique users making 14,999 posts over the last year plus in r/Republican, whereas r/The_Donald has 4,893 unique users to 15,0000 posts over the last 6 days. 86 of those users have posted in both subreddits.\n",
    "    - r/Republican is less active AND has less unique posters. Users posting will probably be a good indicator of which subreddit the post belongs to\n",
    "2. Breitbart articles get posted less on r/Republican, but not as much of a difference as I expected.\n",
    "    - Could indicate heavier alt-right leanings on r/Republican but that is a very broad assumption. There are probably plenty of less famous alt right websites I should check first.\n",
    "3. Trump's name pops up in a LOT of titles on both subreddits; 11.3% of The_Donald titles and 14.8% of Republican titles in the datasets.\n",
    "\n",
    "**For the final model, I will only be using the Title and Body columns to predict the subreddit a post belongs to.**\n",
    "\n",
    "I will need to impute some values for the nulls in the body column before I start manipulating the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update dataframes and append to make one big DF\n",
    "df_don['is_the_donald'] = 1\n",
    "df_rep['is_the_donald'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_rep.append(df_don, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['id', 'score', 'url', 'comms_num', 'created', 'user'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing values with a character that has no sentiment or meaning\n",
    "df['body'] = df['body'].apply(lambda text: '_' if text == '[removed]' or text == '[deleted]' else text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna('_', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SentimentIntensityAnalyzer().polarity_scores(df['title'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['vad_title_neg'] = df['title'].apply(lambda text: SentimentIntensityAnalyzer().polarity_scores(text)['neg'])\n",
    "df['vad_title_neu'] = df['title'].apply(lambda text: SentimentIntensityAnalyzer().polarity_scores(text)['neu'])\n",
    "df['vad_title_pos'] = df['title'].apply(lambda text: SentimentIntensityAnalyzer().polarity_scores(text)['pos'])\n",
    "df['vad_title_compound'] = df['title'].apply(lambda text: SentimentIntensityAnalyzer().polarity_scores(text)['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['vad_body_neg'] = df['body'].apply(lambda text: SentimentIntensityAnalyzer().polarity_scores(text)['neg'])\n",
    "df['vad_body_neu'] = df['body'].apply(lambda text: SentimentIntensityAnalyzer().polarity_scores(text)['neu'])\n",
    "df['vad_body_pos'] = df['body'].apply(lambda text: SentimentIntensityAnalyzer().polarity_scores(text)['pos'])\n",
    "df['vad_body_compound'] = df['body'].apply(lambda text: SentimentIntensityAnalyzer().polarity_scores(text)['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the Data to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined data\n",
    "# df.to_csv('data/data_comb_w_sent.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE: End of PT 1 of EDA. Above code takes a long time to run."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
